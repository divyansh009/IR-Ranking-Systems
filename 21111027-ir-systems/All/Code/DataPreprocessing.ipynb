{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a11c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary packages required for pre-processing:\n",
    "import numpy as np #Used for array operations\n",
    "import nltk #Used as a basic package for nlp operations\n",
    "from nltk.corpus import stopwords #Helps in stop words removal\n",
    "from nltk.stem import WordNetLemmatizer #Helps in lemmatization process\n",
    "from nltk.stem import PorterStemmer #Helps in stemming process\n",
    "#Helps in tokenization of words:\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import math #Used for mathematical equations solving\n",
    "import pickle #Used to dump files to memory so, that we don't need to train model again anad again for a long time.\n",
    "import os #Used to iterate files in the local system\n",
    "import glob #Used for identifying paths in local system\n",
    "import re #Used for removal of non-ascii characters\n",
    "import sys #Used to manipulate different parts of the Python runtime environment\n",
    "from pathlib import Path #Makes it very easy and efficient to deal with file paths\n",
    "from collections import Counter #Used for carrying out frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1f77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=set(stopwords.words('english')) #Storing all stop words in a set data structure\n",
    "ps=PorterStemmer() #Creating object of PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5268b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining certain functions that we'll use in future:\n",
    "#Function to remove non-ascii caracters using regular expression:\n",
    "def remove_non_ascii_characters(data):\n",
    "    pattern=re.compile('[^a-zA-Z0-9\\s]') #Keeping only alphabets and numbers in the text\n",
    "    out=re.sub(pattern,'',data) \n",
    "    return out\n",
    "\n",
    "#Function to find unique words and their frequencies in any document:\n",
    "def find_unique_words_and_freq(data):\n",
    "    unique_words=[] #Initially no unique word\n",
    "    frequency_words={} #Initially frequency list is empty\n",
    "    for x in data:\n",
    "        if x not in unique_words: #If x is not in unique_words list, then append it.\n",
    "            unique_words.append(x)\n",
    "    for x in unique_words: #If x is in unique_words, increment it's frequency count\n",
    "        frequency_words[x] = data.count(x)\n",
    "    return frequency_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a20fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we'll start iterating the files from the given corpus:\n",
    "\n",
    "folder='english-corpora/*' #Specifying location of the text files.\n",
    "ps=PorterStemmer() #Creating an object of porter stemmer\n",
    "\n",
    "words_all=[] #Array for storing all words of corpus\n",
    "words_doc=[] #Array for storing only words of a specific document\n",
    "global_dict={} #Dictionary used to keep unique keys(words) and their counts\n",
    "files_with_index={} #Dictionary for keeping files with index names\n",
    "\n",
    "index=0 #Initializing index value as 0\n",
    "\n",
    "for file in glob.glob(folder): #Iterating all the text files using a loop\n",
    "    a=file #Storing file name as a variable named a\n",
    "    file=open(file,\"r\",encoding='UTF-8') #UTF-8 encoding is used to read all characters of document\n",
    "    \n",
    "    data=file.read() #Reading the file and storing it as a variable named data.\n",
    "    data=remove_non_ascii_characters(data) #Removing non-ascii characters using the defined function above\n",
    "    data=re.sub(re.compile('\\d'),'',data)\n",
    "    \n",
    "    words=word_tokenize(data) #Performing tokenization on the data now and storing it as words.\n",
    "    words=[word for word in words if len(words)>1]\n",
    "    words=[word.lower() for word in words] #Lower casing the words\n",
    "    words=[ps.stem(word) for word in words] #Stemming the words\n",
    "    words=[word for word in words if word not in stop] #Removing stop words \n",
    "    \n",
    "    words_doc.append(words) #Storing words of a specific document in words_doc\n",
    "    \n",
    "    global_dict.update(find_unique_words_and_freq(words)) \n",
    "    #Using function defined above and storing unique \n",
    "    #words and their frequencies in a dictionary.\n",
    "    \n",
    "    files_with_index[index]=os.path.basename(a) #Storing file names as indexes\n",
    "    index=index+1\n",
    "     \n",
    "unique_words_all=set(global_dict.keys()) #We'll store all the unique words in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8184a2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524234"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a298268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf={}\n",
    "df={}\n",
    "for i in unique_words_all: #Iterating all unique words:\n",
    "    tf[i]={} #Making term frequency empty. We'll append document names here containing this word in future.\n",
    "    df[i]=0 #Making document frequency as 0. Means 0 documents have tis word initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24eeab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No, we'll traverse all the text files again and update these term frequency and document frequency. \n",
    "#It will be useful in TF-IDF method later.\n",
    "folder='english-corpora' #Specifying folder name containing files\n",
    "pathlist=Path(folder).rglob('*.txt') #Specifying extension of document names\n",
    "\n",
    "index=0\n",
    "Lavg=0\n",
    "Ltot=0\n",
    "Ld={} #Creating an empty dictionary\n",
    "\n",
    "for path in pathlist: #Traversing all files using loop\n",
    "    fname=str(path) #Storing file names as variable\n",
    "    file=open(fname,\"r\",encoding=\"utf8\") #Opening the file\n",
    "    data=file.read() #Reading the file and storing it as variable.\n",
    "    data=remove_non_ascii_characters(data) #Removing non ascii characters\n",
    "    data=re.sub(re.compile('\\d'),'',data) \n",
    "    words=word_tokenize(data) #Performing word tokenization\n",
    "    words=[word.lower() for word in words] #Lower casing the words\n",
    "    words=[ps.stem(word) for word in words] #Stemming the words\n",
    "    Ld[index]=len(words) #len of current doc\n",
    "    Ltot=Ltot+len(words) #sum of lens of all the docs\n",
    "    words=[word for word in words if word not in stop] #Stop words removal\n",
    "    counter=Counter(words) #Using counter data structure to maintain term frequency count\n",
    "    for i in counter.keys(): #Iterating the unique words\n",
    "        df[i]=df[i]+1 #Incrementing the document frequency by 1\n",
    "        tf[i][index]=counter[i] #Storing ith value of counter in tf[i][index] place.\n",
    "    index=index+1 #Incrementing index counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5a8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving it as posting list using pickle file\n",
    "with open('Saved/posting_list.pkl','wb') as file:\n",
    "    pickle.dump(tf,file)\n",
    "    file.close()    \n",
    "#Saving it as df using pickle file\n",
    "with open('Saved/df.pkl','wb') as file:\n",
    "    pickle.dump(df,file)\n",
    "    file.close()\n",
    "#Saving it as doc_len using pickle file\n",
    "with open('Saved/doc_len.pkl','wb') as file:\n",
    "    pickle.dump(Ld,file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1044c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_norm={}\n",
    "idx=0\n",
    "for i in words_doc: #Traversing all words of any document\n",
    "    l2=0\n",
    "    for j in set(i): #If the word exists in document\n",
    "        l2+=(i.count(j)*math.log(len(files_with_index)/df[j]))**2 #Written formula to store the idf \n",
    "    doc_norm[idx]=(math.sqrt(l2))\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f3223d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving it using pickle file\n",
    "a_file=open(\"Saved/file_idx.pkl\",\"wb\")\n",
    "pickle.dump(files_with_index, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff4f8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving it using pickle file\n",
    "a_file=open(\"Saved/unique_words_all.pkl\",\"wb\")\n",
    "pickle.dump(unique_words_all , a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "911504c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving it using pickle file\n",
    "with open('Saved/doc_words.pkl','wb') as file:\n",
    "    pickle.dump(words_doc,file)\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f5a8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving it using pickle file\n",
    "with open('Saved/doc_norm.pkl','wb') as file:\n",
    "    pickle.dump(doc_norm,file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbf92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
