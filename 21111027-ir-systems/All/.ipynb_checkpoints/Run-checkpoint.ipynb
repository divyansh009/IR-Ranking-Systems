{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "ps=PorterStemmer()\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Generated/posting_list.pkl',\"rb\") as temp:\n",
    "    posting_lists=pickle.load(temp)\n",
    "    temp.close()\n",
    "    \n",
    "with open('../Generated/file_idx.pkl',\"rb\") as temp:\n",
    "    file_idx=pickle.load(temp)\n",
    "    temp.close()\n",
    "\n",
    "unique_words=set(posting_lists.keys())\n",
    "\n",
    "with open('../Generated/df.pkl','rb') as file:\n",
    "    DF=pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "with open('../Generated/doc_words.pkl','rb') as file:\n",
    "    doc_words=pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "with open('../Generated/doc_norm.pkl','rb') as file:\n",
    "    doc_norm=pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "with open('../Generated/doc_len.pkl','rb') as file:\n",
    "    doc_len=pickle.load(file)\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLower(text):\n",
    "    return str(np.char.lower(text))\n",
    "\n",
    "def removeStopWords(words):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    word_list=[]\n",
    "    for w in words:\n",
    "        if w==\"and\" or w==\"or\" or (w not in stop_words):\n",
    "            word_list.append(w)\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def numToWords(words):\n",
    "    for i,w in enumerate(words):\n",
    "        if w.isdigit() and len(w)<4:\n",
    "            words[i]=num2words(int(w))\n",
    "    return words\n",
    "    \n",
    "#stemming/Lemmatize\n",
    "def doStemming(words):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps=PorterStemmer()\n",
    "    \n",
    "    stem_list=[ps.stem(w) for w in words]\n",
    "    return stem_list\n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    text_returned = re.sub(regex,'',text)\n",
    "    return text_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BRS(query,counter=10):\n",
    "    text = remove_special_characters(query)\n",
    "    text = re.sub(re.compile('\\d'),'',text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    words = [word for word in words if word not in Stopwords]\n",
    "    if len(words)>1:\n",
    "        query_words=[words[0]]\n",
    "    else:\n",
    "        query_words=words\n",
    "\n",
    "    for i in range(1,len(words)):\n",
    "        if words[i] not in [\"and\",\"or\"]:\n",
    "            if query_words[-1] not in [\"and\",\"or\"]:\n",
    "                query_words.append(\"and\")\n",
    "                query_words.append(words[i])\n",
    "            else:\n",
    "                query_words.append(words[i])\n",
    "        elif query_words[-1] not in [\"and\",\"or\"]:\n",
    "            query_words.append(words[i])\n",
    "    operators=[]\n",
    "    main_words=[]\n",
    "\n",
    "    for w in query_words:\n",
    "        if w.lower() in [\"and\",\"or\"]:\n",
    "            operators.append(w.lower())\n",
    "        else:\n",
    "            main_words.append(w)\n",
    "    n=len(file_idx)\n",
    "    word_vector=[]\n",
    "    word_vector_matrix=[]\n",
    "\n",
    "    for w in main_words:\n",
    "        word_vector=[0]*n\n",
    "        if w in unique_words:\n",
    "            for x in posting_lists[w].keys():\n",
    "                word_vector[x]=1\n",
    "        word_vector_matrix.append(word_vector)\n",
    "    for w in operators:\n",
    "        vector1=word_vector_matrix[0]\n",
    "        vector2=word_vector_matrix[1]\n",
    "\n",
    "        if w==\"and\":\n",
    "            result=[b1&b2 for b1,b2 in zip(vector1,vector2)]\n",
    "\n",
    "            word_vector_matrix.pop(0)\n",
    "            word_vector_matrix.pop(0)\n",
    "\n",
    "            word_vector_matrix.insert(0,result)\n",
    "        else:\n",
    "            result=[b1|b2 for b1,b2 in zip(vector1,vector2)]\n",
    "\n",
    "            word_vector_matrix.pop(0)\n",
    "            word_vector_matrix.pop(0)\n",
    "\n",
    "            word_vector_matrix.insert(0,result)\n",
    "    final_word_vector=word_vector_matrix[0]\n",
    "    cnt=0\n",
    "    files=[]\n",
    "    for i in final_word_vector:\n",
    "        if i==1:\n",
    "            files.append(file_idx[cnt])\n",
    "            counter-=1\n",
    "        cnt+=1\n",
    "        if counter==0:\n",
    "            break\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(query,counter=10):\n",
    "    text = remove_special_characters(query)\n",
    "    text = re.sub(re.compile('\\d'),'',text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    words=[ps.stem(word) for word in words]\n",
    "    words=[word for word in words if word not in Stopwords]\n",
    "    words=[word for word in words if word in DF.keys()]\n",
    "\n",
    "    q=[]\n",
    "    q_norm=0\n",
    "    for w in words:\n",
    "        tf_idf=(words.count(w)*math.log(len(file_idx)/DF[w]))\n",
    "        q.append(tf_idf)\n",
    "        q_norm+=tf_idf**2\n",
    "    q_norm=math.sqrt(q_norm)\n",
    "\n",
    "    q=np.array(q)/q_norm\n",
    "\n",
    "    score={}\n",
    "\n",
    "    for i in range(len(file_idx)):\n",
    "        doc_v=[]\n",
    "        for w in words:\n",
    "            tf_idf=(doc_words[i].count(w)*math.log(len(file_idx)/DF[w]))\n",
    "            doc_v.append(tf_idf)\n",
    "        doc_v=np.array(doc_v)/doc_norm[i]\n",
    "        score[i]=np.dot(q,doc_v)\n",
    "\n",
    "    score=sorted(score.items(),key=lambda x:x[1],reverse=True)\n",
    "    files=[]\n",
    "    for i in score:\n",
    "        counter-=1\n",
    "        files.append([file_idx[i[0]],i[1]])\n",
    "        if counter == 0:\n",
    "            break\n",
    "            \n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "Ld=doc_len\n",
    "N=len(file_idx)\n",
    "for i in Ld:\n",
    "    k+=Ld[i]\n",
    "Lavg=k/N\n",
    "def IDF(q):\n",
    "    DF1=0\n",
    "    if q in DF:\n",
    "        DF1=DF[q]\n",
    "    ans=math.log((N-DF1+0.5)/(DF1+0.5))\n",
    "    return ans\n",
    "\n",
    "def score_doc(q,score):\n",
    "    q = remove_special_characters(q)\n",
    "    q = re.sub(re.compile('\\d'),'',q)\n",
    "    words = word_tokenize(q)\n",
    "    words = [word.lower() for word in words]\n",
    "    words=[ps.stem(word) for word in words]\n",
    "    words=[word for word in words if word not in Stopwords]\n",
    "    for i in range(len(file_idx)):\n",
    "        score[i]=0\n",
    "        for qi in words:\n",
    "            TF=0\n",
    "            if qi in posting_lists:\n",
    "                if i in posting_lists[qi]:\n",
    "                    TF=posting_lists[qi][i]\n",
    "            idf=IDF(qi)\n",
    "            ans=idf*(k+1)*TF/(TF+k*(1-b+b*(Ld[i]/Lavg)))\n",
    "            score[i]+=ans\n",
    "            \n",
    "\n",
    "k=1.2\n",
    "b=0.75\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(query,counter=10):\n",
    "    score={}\n",
    "    for i in range(len(file_idx)):\n",
    "        score[i]=0\n",
    "    score_doc(query,score)\n",
    "    score=sorted(score.items(),key=lambda item: item[1],reverse=True)\n",
    "    # print(score)\n",
    "    count = 10\n",
    "    files=[]\n",
    "    for i in score:\n",
    "        count-=1\n",
    "        files.append([file_idx[i[0]],i[1]])\n",
    "        if count == 0:\n",
    "            break\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list=pd.read_csv('../Output/query.txt',sep='\\t',header=None)\n",
    "query_list.columns=['qid','query']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv=[]\n",
    "for index, row in query_list.iterrows():\n",
    "    files=BRS(row['query'])\n",
    "    for file in files:\n",
    "        csv.append([row['qid'],1,file,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(csv,columns=['QueryID','Iteration','DocId','Relevance']).to_csv('../Output/BRS.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv=[]\n",
    "for index, row in query_list.iterrows():\n",
    "    files=TFIDF(row['query'])\n",
    "    for file in files:\n",
    "        relevance=0\n",
    "        if file[1]>0:\n",
    "            relevance=1\n",
    "        csv.append([row['qid'],1,file[0],relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(csv,columns=['QueryID','Iteration','DocId','Relevance']).to_csv('../Output/TFIDF.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv=[]\n",
    "for index, row in query_list.iterrows():\n",
    "    files=BM25(row['query'])\n",
    "    for file in files:\n",
    "        relevance=0\n",
    "        if file[1]>0:\n",
    "            relevance=1\n",
    "        csv.append([row['qid'],1,file[0],relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(csv,columns=['QueryID','Iteration','DocId','Relevance']).to_csv('../Output/BM25.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
